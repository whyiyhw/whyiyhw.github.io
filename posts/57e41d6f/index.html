

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="YI">
  <meta name="keywords" content="blog&amp;life">
  
    <meta name="description" content="概念解释 Obsidian: 一种支持多平台的知识管理和笔记应用，它允许用户创建、编辑和链接他们的笔记，支持Markdown格式，可以帮助用户更好地组织和查找他们的知识。   Ollama: 是一个 local AI 工具，它可以在本地运行，并提供与openai 类似的API，无需联网便能实现强大的计算和数据分析功能。   2B&#x2F;7B : 指的是模型的大小，2B 指的是20亿参数模型，7">
<meta property="og:type" content="article">
<meta property="og:title" content="本地大模型在obsidian中的实践">
<meta property="og:url" content="https://blogxy.cn/posts/57e41d6f/index.html">
<meta property="og:site_name" content="积木成楼">
<meta property="og:description" content="概念解释 Obsidian: 一种支持多平台的知识管理和笔记应用，它允许用户创建、编辑和链接他们的笔记，支持Markdown格式，可以帮助用户更好地组织和查找他们的知识。   Ollama: 是一个 local AI 工具，它可以在本地运行，并提供与openai 类似的API，无需联网便能实现强大的计算和数据分析功能。   2B&#x2F;7B : 指的是模型的大小，2B 指的是20亿参数模型，7">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blogxy.cn/img/post/ComfyUI_00006_.png">
<meta property="article:published_time" content="2024-02-26T11:33:32.000Z">
<meta property="article:modified_time" content="2024-03-02T13:05:49.887Z">
<meta property="article:author" content="YI">
<meta property="article:tag" content="ai">
<meta property="article:tag" content="ollama">
<meta property="article:tag" content="openai">
<meta property="article:tag" content="obsidian">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://blogxy.cn/img/post/ComfyUI_00006_.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>本地大模型在obsidian中的实践 - 积木成楼</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blogxy.cn","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>积木成楼</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-exp-fill"></i>
                进行中
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/golang-web/">
                    <i class="iconfont icon-book"></i>
                    golang-web框架解析
                  </a>
                
              </div>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="本地大模型在obsidian中的实践"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-02-26 19:33" pubdate>
          2024年2月26日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          4.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          37 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">本地大模型在obsidian中的实践</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="概念解释"><a href="#概念解释" class="headerlink" title="概念解释"></a>概念解释</h2><blockquote>
<p>Obsidian: 一种支持多平台的知识管理和笔记应用，它允许用户创建、编辑和链接他们的笔记，支持Markdown格式，可以帮助用户更好地组织和查找他们的知识。</p>
</blockquote>
<blockquote>
<p>Ollama: 是一个 local AI 工具，它可以在本地运行，并提供与openai 类似的API，无需联网便能实现强大的计算和数据分析功能。</p>
</blockquote>
<blockquote>
<p><strong>2B</strong>&#x2F;<strong>7B</strong> : 指的是模型的大小，2B 指的是20亿参数模型，7B 指的是70亿参数模型。这些参数决定了模型的复杂度和处理能力。</p>
</blockquote>
<blockquote>
<p>GitHub Copilot : 是一个由GitHub和OpenAI共同开发的人工智能编程助手。它可以根据你的代码和注释，自动生成代码建议，帮助你更快地编写代码。</p>
</blockquote>
<blockquote>
<p>FP16 : 16位浮点数 , 1 字节 8位，所以需要 2字节进行存储。</p>
</blockquote>
<h2 id="0x01-起源"><a href="#0x01-起源" class="headerlink" title="0x01 起源"></a>0x01 起源</h2><p>本篇将会介绍，如何在 Obsidian 中，通过 <code>ollama</code> &#x2F; <code>openai</code> 的能力进行文章编写。这个想法的来源是 GitHub Copilot 中的 idea。</p>
<p>在软件开发中，GitHub Copilot 已经展示了其强大的编程辅助能力，那么为何我们不能在文章编写中也利用到类似的技术呢？这就是我想在 Obsidian 中利用 ollama 或 openai 能力进行快速文章编写的初衷。</p>
<p>首先，我们在 GitHub 上找到了一个 插件 <a target="_blank" rel="noopener" href="https://github.com/pfrankov/obsidian-local-gpt">Local GPT</a>提供了 Obsidian 链接 到 <code>OpenAI</code>&amp;<code>ollama</code> 的功能，这使我们可以直接在 Obsidian 中调用AI的能力。这个插件的使用非常简单，只需要简单的安装和配置，就可以开始我们的AI写作旅程了。</p>
<h2 id="0x02-本地modelVSopenai"><a href="#0x02-本地modelVSopenai" class="headerlink" title="0x02 本地modelVSopenai"></a>0x02 本地<code>model</code>VS<code>openai</code></h2><blockquote>
<p>最大的问题就在于<strong>隐私和数据安全</strong>这一方面。虽然OpenAI已经采取了严格的数据安全措施，但数据仍然需要通过网络发送，而OpenAI本身也会存储这些数据。</p>
</blockquote>
<blockquote>
<p><strong>成本</strong>是另一个需要考虑的问题。当本地模型的硬件足够时，我们最多需要考虑的是电费，但OpenAI 的 gpt4 api 的价格并不便宜。</p>
</blockquote>
<blockquote>
<p>在谈到<strong>可定制性</strong>时，这需要你具备一定的开发能力，以及对硬件设备、数据采集和模型训练的经验。然而，随着开源产品的不断迭代和改进，这种要求将会变得更为普遍且操作更为简便。</p>
</blockquote>
<h3 id="ollama"><a href="#ollama" class="headerlink" title="ollama"></a>ollama</h3><p>通过 <a target="_blank" rel="noopener" href="https://ollama.ai/">https://ollama.ai</a> 下载安装，然后根据你的电脑配置选择合适的模型。</p>
<blockquote>
<p>实际上在装好 ollama 之后，它本身也只是一个模型推理服务上，为了更加直观的使用，推荐与其配套的一个前端，<a target="_blank" rel="noopener" href="https://github.com/open-webui/open-webui">https://github.com/open-webui/open-webui</a>。就目前的体验来说，已经很不错了, 除了 docker 运行容器有点大以外。</p>
</blockquote>
<h4 id="如何-在-ollama-中给你的电脑选择合适的模型？"><a href="#如何-在-ollama-中给你的电脑选择合适的模型？" class="headerlink" title="如何 在 ollama 中给你的电脑选择合适的模型？"></a>如何 在 ollama 中给你的电脑选择合适的模型？</h4><h5 id="模型大小跟显存大小的关系是什么？"><a href="#模型大小跟显存大小的关系是什么？" class="headerlink" title="模型大小跟显存大小的关系是什么？"></a>模型大小跟显存大小的关系是什么？</h5><blockquote>
<p>首先要回答这么一个问题，FP16的模型，每增加1B参数量需要增加 2GB 的显存去加载是怎么推算出来的？</p>
</blockquote>
<p>在深度学习模型中，模型的参数通常以权重的形式存储。每个权重通常是一个浮点数，一般使用16位浮点数，那么每个权重就需要16位，即2字节的存储空间。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>,<span class="hljs-number">000</span>,<span class="hljs-number">000</span>,<span class="hljs-number">000</span> x <span class="hljs-number">2</span> (字节/byte)  / <span class="hljs-number">1024</span> = <span class="hljs-number">1</span>,<span class="hljs-number">953</span>,<span class="hljs-number">125</span> (KB) <br><span class="hljs-attribute">1</span>,<span class="hljs-number">953</span>,<span class="hljs-number">125</span> / <span class="hljs-number">1024</span> = <span class="hljs-number">1</span>,<span class="hljs-number">907</span>.<span class="hljs-number">3486328125</span> (MB)<br><span class="hljs-attribute">1</span>,<span class="hljs-number">907</span>.<span class="hljs-number">3486328125</span> / <span class="hljs-number">1024</span> = <span class="hljs-number">1</span>.<span class="hljs-number">8626451</span> (GB)<br></code></pre></td></tr></table></figure>

<p>也就是说使用 fb16 的模型，每1B的模型就需要 2GB 的显存。</p>
<h5 id="什么是模型量化？"><a href="#什么是模型量化？" class="headerlink" title="什么是模型量化？"></a>什么是模型量化？</h5><p>搞清楚 模型大小跟显存大小的关系后，这个问题之后 ，我们来接受一个概念 <strong>模型量化</strong>，既然用 fb16 能存数据，那么我们用更小的数据类型是不是也能存，通过减少模型权重的精度，从而减小模型大小和运行时间，这就是 <strong>模型量化</strong> 。</p>
<p>模型量化技术通常将浮点数权重转换为定点数权重。例如，原本需要16位精度（即float16）的权重，经过量化后，通常会损失一些精度，但可能只需要8位精度（即int8）就足够了。这样的话，模型的大小就可以减小到原来的1&#x2F;2，而且计算速度也会显著提高。</p>
<p>但是 <strong>量化会导致模型的准确性下降</strong>  ， 目前大模型中性价比最高的是，量化 Q4 也就是将 FP16 转为 int4 , 单个参数的大小降低为原来的1&#x2F;4, 理论上模型加载耗费也为原来的 1&#x2F;4 。</p>
<p>让我们来大致预估计算下 7B模型，量化Q4 需要多少显存才能运行</p>
<blockquote>
<p><code>7B * 2 = 14GB  ，14GB * 1/4 = 3.5GB</code><br><code>13B * 2 = 26GB  ，28GB * 1/4 = 6.5GB</code> </p>
</blockquote>
<p>所以理论上本地只需要 4G 显存就能运行7B-Q4 的模型，8G 就能运行 13B-Q4 模型</p>
<h5 id="我的电脑是什么配置？"><a href="#我的电脑是什么配置？" class="headerlink" title="我的电脑是什么配置？"></a>我的电脑是什么配置？</h5><blockquote>
<p>显卡 1060 6GB , CPU I5-8400 , 内存 ddr3 2400 32GB</p>
</blockquote>
<p>所以理论上，我能加载可以选择 运行 7B 的模型，想想还有点小开心~</p>
<p>接下来进入实践，我去找了下相关模型的 Q4 量化版本，发现模型大小一般都在 3.8GB 起 比如</p>
<ul>
<li><code>llama2:7b-chat-q4_0</code> -&gt; <strong>3.8GB</strong></li>
<li><code>mistral:7b-instruct-q4_0</code> -&gt; <strong>4.1GB</strong></li>
<li><code>llava:7b-v1.5-q4_0</code> -&gt; <strong>4.5GB</strong></li>
<li><code>qwen:7b-chat-v1.5-q4_0</code> -&gt; <strong>4.5GB</strong></li>
<li><code>gemma:7b-instruct-q4_0</code> -&gt; <strong>5.2GB</strong></li>
</ul>
<p>这是因为模型的量级不可能是固定的70亿，在这个基础上浮动也很正常，对外统一宣称7B，再加上附加的其他参数，多1-2G 也很正常。</p>
<p>我进行了本地测试进行验证，结果如下</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>大小</th>
<th>推理速度(tokens&#x2F;s)</th>
<th>是否可用</th>
</tr>
</thead>
<tbody><tr>
<td>gemma:2b</td>
<td>1.7 GB</td>
<td>61.29</td>
<td>✔</td>
</tr>
<tr>
<td>qwen:4b-chat-v1.5-q4_0</td>
<td>2.3 GB</td>
<td>40.5</td>
<td>✔</td>
</tr>
<tr>
<td>qwen:4b-chat-v1.5-q5_1</td>
<td>3.0 GB</td>
<td>36.54</td>
<td>✔</td>
</tr>
<tr>
<td>qwen:7b-chat-v1.5-q2_K</td>
<td>3.1GB</td>
<td>22.03</td>
<td>✔</td>
</tr>
<tr>
<td>yi:6b</td>
<td>3.5 GB</td>
<td>32.38</td>
<td>✔</td>
</tr>
<tr>
<td>gemma:7b-instruct-q2_K</td>
<td>3.7 GB</td>
<td>12.58</td>
<td>✔</td>
</tr>
<tr>
<td>llama2:7b-chat-q4_0</td>
<td>3.8 GB</td>
<td>14.54</td>
<td>✔</td>
</tr>
<tr>
<td>gemma:7b</td>
<td>5.2GB</td>
<td>6.64</td>
<td>✔</td>
</tr>
</tbody></table>
<ul>
<li>注，当模型大小大于 5.2 GB 后，1060 可能就会出现无法加载的问题了，另外 推理速度的快慢对于实际应用非常重要，可以不对，但是慢的话，就真不如使用远程 gpt4 的 api，对于这个点，我个人的感觉是 模型的推理速度低于 20 tokens&#x2F;s 时，用户就会明显感受到延迟，特别是被集成使用的时候，需要 应用对大模型的响应速度进行妥协设计。</li>
</ul>
<h5 id="如何去根据模型大小跟设备情况去推算-tokens-的预计值呢？"><a href="#如何去根据模型大小跟设备情况去推算-tokens-的预计值呢？" class="headerlink" title="如何去根据模型大小跟设备情况去推算 tokens 的预计值呢？"></a>如何去根据模型大小跟设备情况去推算 tokens 的预计值呢？</h5><ul>
<li>分几种情况 只用 GPU，只用 CPU 与混合使用，以 Nvidia 的 GeForce GTX 1060 显卡为例，其显存带宽为 <strong>192GB&#x2F;s</strong>。而本地 DDR3 2400 内存条的理论值是 <strong>19.2 GB&#x2F;s</strong> 的带宽</li>
<li>在推理过程中，模型首先被加载到内存中，然后每生成一个 token，我们都需要从内存中遍历一遍模型。因此，模型越大，生成每个 token 所需的时间就越长。</li>
<li><code>192 / 1.7 = 112</code> 是预计 gemma:2b 的速度值，但是实际上只能跑到 61.29 ，也就是说我本地电脑，在各种偏差后 只能达到 预计值的 54% 左右，当加载的模型超过 3GB 时，这个比例可能会降低到预计值的 30%，而当模型超过 5GB 时，可能会进一步降低到预计值的 16.6%。</li>
<li>而实际观察 cpu 跟内存的负载，在推理时，有了明显的增加，可以认为其也参与了计算，所以 我的理解为 ollama 在Windows 上使用了 gpu跟 cpu 混合计算来进行推理</li>
</ul>
<p>跟推理计算直接关联的变量为，<strong>设备类型</strong>（GPU,CPU,）内存、显存大小跟其对应的带宽大小，还有 <strong>模型大小</strong> ，随着 上下文 token 变大，其推理速度也会变小。</p>
<p>最终我在本地 1060显卡的配置下， 选择的模型是 yi:6b 跟 qwen:4b , 而根据一些实践者的观点 60B 模型才是 ai 有理解能力的关键节点，就算 Q4 量化，也需要 30GB 的显存才能流畅的运行，虽然拿 cpu + 内存也不是不能跑，但是就这个运行效率估计就 1-2 token 的数据 实在难以接受。</p>
<p>如果你的显存是6G以下，最好选择4B-Q4左右的模型，而如果你有8G或12G的显存，那么7B的模型对你来说已经没有什么压力了。</p>
<p>至于模型的能力，确实是一分算力一分货。模型的大小与训练时的文本质量，直接影响了生成的文本质量，更大的模型通常能生成更准确，更自然的文本。</p>
<h3 id="OpenAI"><a href="#OpenAI" class="headerlink" title="OpenAI"></a>OpenAI</h3><p>对于<code>OpenAI</code>，我们需要通过其API进行调用，并且需要注意API的调用次数和费用。我们也需要注意个人敏感信息的安全。虽然 OpenAI 的计算能力强大，但使用它需要一定的编程基础和对API 的使用与限制（内容安全，上下文大小，prompt 工程）有所了解。由于它是云端服务，所以在数据安全方面也存在一定的风险。</p>
<p>我本身是通过 <code>GitHub Copilot</code> 购买了一年的服务，然后通过 <a target="_blank" rel="noopener" href="https://github.com/whyiyhw/copilot-gpt4-service">https://github.com/whyiyhw/copilot-gpt4-service</a> 开放了 <code>GPT-4 API</code>，因此我可以无限制地使用API，而不用担心账单问题。我本身也阅读了 <code>Local GPT</code> 的源码，并确认了使用它是无风险的。其他使用者需要自行关注这些点。</p>
<h2 id="0x03-如何使用"><a href="#0x03-如何使用" class="headerlink" title="0x03 如何使用"></a>0x03 如何使用</h2><p>一旦你完成了上述的安装和配置，你就可以开始你的AI协作编辑之旅了。</p>
<p>我自己定义了一些常用的功能，这些功能可以通过 <code>Local GPT &gt; Add new manually</code> 来加入。这些功能包括<strong>续写</strong>，<strong>总结</strong>，<strong>翻译</strong>，<strong>语句语法检查</strong>，<strong>自动给文章加标签</strong>，以及在 obsidian 中特色的<strong>出链</strong>，<strong>画 Mermaid 图</strong>，和<strong>从文本中列出任务</strong>等。</p>
<p>接下来，我将简单介绍其中的两个功能。</p>
<h3 id="续写"><a href="#续写" class="headerlink" title="续写"></a>续写</h3><p>你可以在 Obsidian 中创建一个新的文档，然后开始输入你的想法或者草稿，接着，你只需要按下特定的快捷键（比如<code>alt + \</code> 我定义的），ai 就会自动为你生成一段接续的文本， 并且这段文本会紧接着你的原始内容，形成一个完整的故事或者观点。这样你就可以无缝地继续你的写作，而不需要担心受到创作阻塞的困扰。<br>【ps: 🌟其实算是偷懒，大致写出骨架，AI 补全剩下的内容, 不用太关注细节，更加流畅。】</p>
<p><img src="/img/post/article_ollama_obsidian_01.png" srcset="/img/loading.gif" lazyload alt="/img/post/article_ollama_obsidian_01"></p>
<h3 id="事实性错误纠正"><a href="#事实性错误纠正" class="headerlink" title="事实性错误纠正"></a>事实性错误纠正</h3><blockquote>
<p>写完后让ai检查文章中的错误，相当于 ai 的校正编辑</p>
</blockquote>
<p><img src="/img/post/article_ollama_obsidian_03.png" srcset="/img/loading.gif" lazyload alt="/img/post/article_ollama_obsidian_03"></p>
<h3 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h3><blockquote>
<p>通过对指定部分的选中，你可以将其翻译为任何你需要的语言。<br>By selecting the specified part, you can translate it into any language you need.</p>
</blockquote>
<p><img src="/img/post/article_ollama_obsidian_02.png" srcset="/img/loading.gif" lazyload alt="/img/post/article_ollama_obsidian_02"></p>
<h2 id="0x04-结论"><a href="#0x04-结论" class="headerlink" title="0x04 结论"></a>0x04 结论</h2><ul>
<li>对比 2023 年的模型遥不可及，到 2024 年的本地小模型可以运行了，期待 1-2年后，可能每个人电脑上都能运行 跟 gpt3.5 同等水平的 ai 。</li>
<li>利用 AI 技术，极大地增强了 Obsidian 作为文本编辑器的能力。</li>
<li>最重要的是，这可以提升我持续输出的能力。<strong>write less , do more</strong>。</li>
</ul>
<h2 id="0x05-扩展小问题？"><a href="#0x05-扩展小问题？" class="headerlink" title="0x05 扩展小问题？"></a>0x05 扩展小问题？</h2><ol>
<li><code>ollama</code> 和 <code>openai</code> 有什么区别和优势，该如何选择？</li>
<li>我可以在本地运行哪些模型？如何选择适合我电脑配置的模型？</li>
<li>如何在 Obsidian 中使用 AI 功能进行文章编写？</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/ai/" class="category-chain-item">ai</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/ai/">#ai</a>
      
        <a href="/tags/ollama/">#ollama</a>
      
        <a href="/tags/openai/">#openai</a>
      
        <a href="/tags/obsidian/">#obsidian</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>本地大模型在obsidian中的实践</div>
      <div>https://blogxy.cn/posts/57e41d6f/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>YI</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年2月26日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/3139d2fc/" title="laravel-horizon原理与实践">
                        <span class="hidden-mobile">laravel-horizon原理与实践</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    

  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
      <div class="col-lg-7 mx-auto nopadding-x-md">
        <div class="container custom mx-auto">
          <img src="https://github.com/whyiyhw/chatgpt-wechat/raw/main/doc/image99.png" srcset="/img/loading.gif" lazyload class="rounded mx-auto d-block mt-5" style="width:150px; height:150px;">
        </div>
      </div>
    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
