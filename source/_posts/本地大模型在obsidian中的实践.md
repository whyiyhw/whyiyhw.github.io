---
title: 本地大模型在obsidian中的实践
tags:
  - ai
  - ollama
  - openai
  - obsidian
categories: ai
abbrlink: 57e41d6f
date: 2024-02-26 19:33:32
index_img: /img/post/ComfyUI_00006_.png
---

## 概念解释
> Obsidian: 一种支持多平台的知识管理和笔记应用，它允许用户创建、编辑和链接他们的笔记，支持Markdown格式，可以帮助用户更好地组织和查找他们的知识。

>Ollama: 是一个 local AI 工具，它可以在本地运行，并提供与openai 类似的API，无需联网便能实现强大的计算和数据分析功能。

>**2B**/**7B** : 指的是模型的大小，2B 指的是20亿参数模型，7B 指的是70亿参数模型。这些参数决定了模型的复杂度和处理能力。

> GitHub Copilot : 是一个由GitHub和OpenAI共同开发的人工智能编程助手。它可以根据你的代码和注释，自动生成代码建议，帮助你更快地编写代码。

> FP16 : 16位浮点数 , 1 字节 8位，所以需要 2字节进行存储。
## 0x01 起源

本篇将会介绍，如何在 Obsidian 中，通过 `ollama` / `openai` 的能力进行文章编写。这个想法的来源是 GitHub Copilot 中的 idea。

在软件开发中，GitHub Copilot 已经展示了其强大的编程辅助能力，那么为何我们不能在文章编写中也利用到类似的技术呢？这就是我想在 Obsidian 中利用 ollama 或 openai 能力进行快速文章编写的初衷。

首先，我们在 GitHub 上找到了一个 插件 [Local GPT](https://github.com/pfrankov/obsidian-local-gpt)提供了 Obsidian 链接 到 `OpenAI`&`ollama` 的功能，这使我们可以直接在 Obsidian 中调用AI的能力。这个插件的使用非常简单，只需要简单的安装和配置，就可以开始我们的AI写作旅程了。

## 0x02 本地`model`VS`openai`

> 最大的问题就在于**隐私和数据安全**这一方面。虽然OpenAI已经采取了严格的数据安全措施，但数据仍然需要通过网络发送，而OpenAI本身也会存储这些数据。

> **成本**是另一个需要考虑的问题。当本地模型的硬件足够时，我们最多需要考虑的是电费，但OpenAI 的 gpt4 api 的价格并不便宜。

> 在谈到**可定制性**时，这需要你具备一定的开发能力，以及对硬件设备、数据采集和模型训练的经验。然而，随着开源产品的不断迭代和改进，这种要求将会变得更为普遍且操作更为简便。
### ollama

通过 [https://ollama.ai](https://ollama.ai/) 下载安装，然后根据你的电脑配置选择合适的模型。

> 实际上在装好 ollama 之后，它本身也只是一个模型推理服务上，为了更加直观的使用，推荐与其配套的一个前端，[https://github.com/open-webui/open-webui](https://github.com/open-webui/open-webui)。就目前的体验来说，已经很不错了, 除了 docker 运行容器有点大以外。

#### 如何 在 ollama 中给你的电脑选择合适的模型？

##### 模型大小跟显存大小的关系是什么？

> 首先要回答这么一个问题，FP16的模型，每增加1B参数量需要增加 2GB 的显存去加载是怎么推算出来的？

在深度学习模型中，模型的参数通常以权重的形式存储。每个权重通常是一个浮点数，一般使用16位浮点数，那么每个权重就需要16位，即2字节的存储空间。

```
1,000,000,000 x 2 (字节/byte)  / 1024 = 1,953,125 (KB) 
1,953,125 / 1024 = 1,907.3486328125 (MB)
1,907.3486328125 / 1024 = 1.8626451 (GB)
```

也就是说使用 fb16 的模型，每1B的模型就需要 2GB 的显存。

##### 什么是模型量化？

搞清楚 模型大小跟显存大小的关系后，这个问题之后 ，我们来接受一个概念 **模型量化**，既然用 fb16 能存数据，那么我们用更小的数据类型是不是也能存，通过减少模型权重的精度，从而减小模型大小和运行时间，这就是 **模型量化** 。

模型量化技术通常将浮点数权重转换为定点数权重。例如，原本需要16位精度（即float16）的权重，经过量化后，通常会损失一些精度，但可能只需要8位精度（即int8）就足够了。这样的话，模型的大小就可以减小到原来的1/2，而且计算速度也会显著提高。

但是 **量化会导致模型的准确性下降**  ， 目前大模型中性价比最高的是，量化 Q4 也就是将 FP16 转为 int4 , 单个参数的大小降低为原来的1/4, 理论上模型加载耗费也为原来的 1/4 。

让我们来大致预估计算下 7B模型，量化Q4 需要多少显存才能运行
> `7B * 2 = 14GB  ，14GB * 1/4 = 3.5GB` 
> `13B * 2 = 26GB  ，28GB * 1/4 = 6.5GB` 

所以理论上本地只需要 4G 显存就能运行7B-Q4 的模型，8G 就能运行 13B-Q4 模型
##### 我的电脑是什么配置？
> 显卡 1060 6GB , CPU I5-8400 , 内存 ddr3 2400 32GB

所以理论上，我能加载可以选择 运行 7B 的模型，想想还有点小开心~

接下来进入实践，我去找了下相关模型的 Q4 量化版本，发现模型大小一般都在 3.8GB 起 比如

- `llama2:7b-chat-q4_0` -> **3.8GB**
- `mistral:7b-instruct-q4_0` -> **4.1GB**
- `llava:7b-v1.5-q4_0` -> **4.5GB**
- `qwen:7b-chat-v1.5-q4_0` -> **4.5GB**
- `gemma:7b-instruct-q4_0` -> **5.2GB**

这是因为模型的量级不可能是固定的70亿，在这个基础上浮动也很正常，对外统一宣称7B，再加上附加的其他参数，多1-2G 也很正常。

我进行了本地测试进行验证，结果如下

| 模型                     | 大小     | 推理速度(tokens/s) | 是否可用 |
| ---------------------- | ------ | -------------- | ---- |
| gemma:2b               | 1.7 GB | 61.29          | ✔    |
| qwen:4b-chat-v1.5-q4_0 | 2.3 GB | 40.5           | ✔    |
| qwen:4b-chat-v1.5-q5_1 | 3.0 GB | 36.54          | ✔    |
| qwen:7b-chat-v1.5-q2_K | 3.1GB  | 22.03          | ✔    |
| yi:6b                  | 3.5 GB | 32.38          | ✔    |
| gemma:7b-instruct-q2_K | 3.7 GB | 12.58          | ✔    |
| llama2:7b-chat-q4_0    | 3.8 GB | 14.54          | ✔    |
| gemma:7b               | 5.2GB  | 6.64           | ✔    |

- 注，当模型大小大于 5.2 GB 后，1060 可能就会出现无法加载的问题了，另外 推理速度的快慢对于实际应用非常重要，可以不对，但是慢的话，就真不如使用远程 gpt4 的 api，对于这个点，我个人的感觉是 模型的推理速度低于 20 tokens/s 时，用户就会明显感受到延迟，特别是被集成使用的时候，需要 应用对大模型的响应速度进行妥协设计。
##### 如何去根据模型大小跟设备情况去推算 tokens 的预计值呢？
- 分几种情况 只用 GPU，只用 CPU 与混合使用，以 Nvidia 的 GeForce GTX 1060 显卡为例，其显存带宽为 **192GB/s**。而本地 DDR3 2400 内存条的理论值是 **19.2 GB/s** 的带宽
- 在推理过程中，模型首先被加载到内存中，然后每生成一个 token，我们都需要从内存中遍历一遍模型。因此，模型越大，生成每个 token 所需的时间就越长。
- `192 / 1.7 = 112` 是预计 gemma:2b 的速度值，但是实际上只能跑到 61.29 ，也就是说我本地电脑，在各种偏差后 只能达到 预计值的 54% 左右，当加载的模型超过 3GB 时，这个比例可能会降低到预计值的 30%，而当模型超过 5GB 时，可能会进一步降低到预计值的 16.6%。
- 而实际观察 cpu 跟内存的负载，在推理时，有了明显的增加，可以认为其也参与了计算，所以 我的理解为 ollama 在Windows 上使用了 gpu跟 cpu 混合计算来进行推理

跟推理计算直接关联的变量为，**设备类型**（GPU,CPU,）内存、显存大小跟其对应的带宽大小，还有 **模型大小** ，随着 上下文 token 变大，其推理速度也会变小。

最终我在本地 1060显卡的配置下， 选择的模型是 yi:6b 跟 qwen:4b , 而根据一些实践者的观点 60B 模型才是 ai 有理解能力的关键节点，就算 Q4 量化，也需要 30GB 的显存才能流畅的运行，虽然拿 cpu + 内存也不是不能跑，但是就这个运行效率估计就 1-2 token 的数据 实在难以接受。

如果你的显存是6G以下，最好选择4B-Q4左右的模型，而如果你有8G或12G的显存，那么7B的模型对你来说已经没有什么压力了。

至于模型的能力，确实是一分算力一分货。模型的大小与训练时的文本质量，直接影响了生成的文本质量，更大的模型通常能生成更准确，更自然的文本。

### OpenAI

对于`OpenAI`，我们需要通过其API进行调用，并且需要注意API的调用次数和费用。我们也需要注意个人敏感信息的安全。虽然 OpenAI 的计算能力强大，但使用它需要一定的编程基础和对API 的使用与限制（内容安全，上下文大小，prompt 工程）有所了解。由于它是云端服务，所以在数据安全方面也存在一定的风险。

我本身是通过 `GitHub Copilot` 购买了一年的服务，然后通过 [https://github.com/whyiyhw/copilot-gpt4-service](https://github.com/whyiyhw/copilot-gpt4-service) 开放了 `GPT-4 API`，因此我可以无限制地使用API，而不用担心账单问题。我本身也阅读了 `Local GPT` 的源码，并确认了使用它是无风险的。其他使用者需要自行关注这些点。

## 0x03 如何使用

一旦你完成了上述的安装和配置，你就可以开始你的AI协作编辑之旅了。

我自己定义了一些常用的功能，这些功能可以通过 `Local GPT > Add new manually` 来加入。这些功能包括**续写**，**总结**，**翻译**，**语句语法检查**，**自动给文章加标签**，以及在 obsidian 中特色的**出链**，**画 Mermaid 图**，和**从文本中列出任务**等。

接下来，我将简单介绍其中的两个功能。
### 续写

你可以在 Obsidian 中创建一个新的文档，然后开始输入你的想法或者草稿，接着，你只需要按下特定的快捷键（比如`alt + \` 我定义的），ai 就会自动为你生成一段接续的文本， 并且这段文本会紧接着你的原始内容，形成一个完整的故事或者观点。这样你就可以无缝地继续你的写作，而不需要担心受到创作阻塞的困扰。
【ps: 🌟其实算是偷懒，大致写出骨架，AI 补全剩下的内容, 不用太关注细节，更加流畅。】

![/img/post/article_ollama_obsidian_01](/img/post/article_ollama_obsidian_01.png)

### 事实性错误纠正

>写完后让ai检查文章中的错误，相当于 ai 的校正编辑

![/img/post/article_ollama_obsidian_03](/img/post/article_ollama_obsidian_03.png)

### 翻译

> 通过对指定部分的选中，你可以将其翻译为任何你需要的语言。
> By selecting the specified part, you can translate it into any language you need.

![/img/post/article_ollama_obsidian_02](/img/post/article_ollama_obsidian_02.png)

## 0x04 结论

- 对比 2023 年的模型遥不可及，到 2024 年的本地小模型可以运行了，期待 1-2年后，可能每个人电脑上都能运行 跟 gpt3.5 同等水平的 ai 。
- 利用 AI 技术，极大地增强了 Obsidian 作为文本编辑器的能力。
- 最重要的是，这可以提升我持续输出的能力。**write less , do more**。

## 0x05 扩展小问题？

1. `ollama` 和 `openai` 有什么区别和优势，该如何选择？
2. 我可以在本地运行哪些模型？如何选择适合我电脑配置的模型？
3. 如何在 Obsidian 中使用 AI 功能进行文章编写？